---
title: "Phenomenal Eevee"
subtitle: "Report"
format: html
editor: visual
execute:
  echo: true
---

# Introduction

Streaming platforms have taken the world by storm due to their convenience and ability to house innumerable popular television shows and movies. As these mediums continue to grow in popularity, discourse about them is also developing and is often centered around one primary question: Which platform is best? Throughout our data analysis, our team seeks to answer this aforementioned question to the best of our ability, taking an in-depth look at four mainstream services ---- Netflix, Amazon Prime Video, Hulu, and Disney+ ---- and the programs that have been present on the site throughout roughly the past two decades. Specifically, we use rankings developed by Rotten Tomatoes and hypothesis testing to approach two research questions:

**RQ1: "Is the quality of exclusive shows different from non-exclusive shows in each respective platform?"**

**RQ2: "Are exclusive shows' ratings different between the four platforms (Netflix, Hulu, Disney+, and Prime Video)?"**

Regarding the first hypothesis, we were unable to reject the null hypothesis: the quality of exclusive shows (measured by Rotten Tomatoes scores) do not appear to be different from that of non-exclusive shows on each respective platform. In terms of the second research question, our findings were in favor of the alternate hypothesis: there is some sort of difference in ratings between the exclusive shows on each of the four platforms. Specifically,

# Data description

The observations (rows) of the `tvshows` dataset represent each individual TV show that is readily watched and broadcasted on at least one of the four top ranking broadcasting platforms (we are focusing on Hulu, Disney+, Prime Video and Netflix). The attributes (columns) of the `tvshows` data set categorizes each individual TV show based on the streaming platform it's being broadcasted on and reveals additional information. This includes revealing the year the TV show was produced, target age group of audience, and the rating of the show (by the Rotten Tomatoes metric).

This data set was created by the curator after being inspired by personal experiences of wanting to know more about which streaming platform(s) a particular TV show can be found on. Furthermore, the curator wished to explore potential relationships between target age group of audience, year of production, and the streaming platform the show can be found on. The creation of the `tvshows` dataset was not funded by anyone and was curated solely with the desire to learn more about tv shows readily broadcasted on top streaming platforms. Specifically, the curator likely looked at TV shows broadcasted on each streaming platform to determine which Rotten Tomatoes ratings to include within the dataset and determine which year of production data as well as target age of audience should be included.

In regards to the pre-processing of the data, the Rotten Tomatoes score itself needed to undergo specific calculations before the curator is able to scrape the Rotten Tomatoes score for the corresponding TV shows. To summarize, the Rotten Tomatoes score is calculated only when the show receives at least 5 reviews. Then, the critic's rating of "Fresh" (positive) or "Rotten" bad is collected and the Rotten Tomatoes percentage is calculated by dividing the number of "Fresh" scores by the total number of "Fresh" and "Rotten" scores times 100. In addition, the curator then used a binary system (0 for "no" and 1 for "yes") to classify which streaming platform(s) each TV show can be found on.

The curator did not involve any other people during the data collection and likely collected the data from official sites that reported the taken corresponding data values. At the same time, the Rotten Tomatoes data values inherently involve people as the score is reliant on people's opinions and rating of the show. From this perspective, the people are likely aware of this data collection as they are displaying their ratings of the tv shows in a public domain. They likely expected their data to be used to contribute to the overall rating of the TV show and used to inform others on whether they should watch the respective TV show.

# Data analysis

```{r}
#| label: data-cleaning-for-exclusivity

tv_shows_cleaned

```

```{r}
#| label: explaratory-data-analysis

total_per_platform |>
  ggplot(aes(x = decile, y = number, fill = platform)) +
  geom_bar(position = "fill", stat = "identity") +
  theme_minimal() +
  labs(
    title = "Fraction of shows that are in a given rating interval for each platform",
    x = "Rating intervals", y = "Fraction of shows", 
    fill = "Platforms") +
  scale_fill_manual(
    values = viridis(4),
    labels = c("Disney+", "Hulu", "Netflix", "Prime Video")
  )

total |> 
  ggplot(aes(x = platform, y = rotten_tomatoes, fill = platform)) + 
  geom_boxplot(show.legend =  FALSE) +
  labs(title = "Ratings of Streamed Shows by Platform", 
     x = "Platforms", y = "Show Ratings", disney="Disney+") + 
  scale_fill_brewer(palette = "Spectral") +
  scale_x_discrete(labels = c("Disney+", "Hulu", "Netflix", "Prime Video"))

mean_rankings |>
  ggplot(aes(x = platform, y = mean, fill = platform)) + ylim(c(0, 100)) +
  geom_bar(stat = "identity", show.legend =  FALSE) +
  labs(title = "Mean Ratings of Streamed Shows by Platform",
       x = "Platforms", y = "Mean Ratings") +
  scale_fill_brewer(palette = "Dark2")

```

**Fraction of shows that are in a given rating interval for each platform:**

The graph titled *Fraction of shows that are in a given rating interval for each platform* is a normalized frequency bar chart where every bar has the same height so that each bar represents the proportion of each rating (in our case: rotten tomatoes score) interval for a given platform. This allows us to normalize the number of shows in each platform and reduce the bias of different platforms having different number of shows. In this graph, the y-axis represents the fraction of shows (compared to the overall number of shows) rather than the actual number of shows on each platform. This allows us to compare the rotten tomato scores of shows of each platform as well as across all the platforms. The x-axis represents the rotten tomato score intervals (decided) that we chose to categorize and group the shows based on.

Based on the graph we can deduce that Hulu and Netflix have equally large proportion of shows ranked between 91 and 100 on rotten tomatoes. Prime Video has the third largest proportion of shows in that interval of rotten tomatoes scores, followed by the last platform, which is Disney+. The same tendency is consistent for rotten tomatoes score intervals 51-60, 61-70, 71-80, and 81-90 rotten tomatoes score intervals. The proportion of shows that have the lowest rotten tomatoes score is the largest for Prime Video. Shows ranked within the interval of 1-10 are mostly streamed on Prime Video. In general, it seems like Netflix and Hulu have the largest proportion of shows that are ranked relatively high on rotten tomatoes, meanwhile Prime Video has the largest proportion of shows that are ranked the lowest on rotten tomatoes.

**Ratings of Streamed Shows by Platform:**

This graph shows boxplots that represent the distribution of rotten tomatoes scores of shows on each platform. The heights of the boxplots represent the diversity of data points (in this case rotten tomatoes scores) on each platform. The rotten tomatoes scores of shows in Disney+ are the most consistent across all the shows and are similar to each other with the median rotten tomatoes score being 50 (indicated by the horizontal line). The same is true for scores of shows streaming on Netflix. On the other hand, because the boxplot for shows streaming on Prime Video is well spread out across the y-axis, meaning its IQR is large, the scores for the shows streaming on Prime Video have varying range and are not consistent or similar across all the shows on the platform. This also means that even though there is a mean value of around 40, we cannot make meaningful conclusions about Prime Video show scores that would be relevant and valid for all the shows across the platform. This said, comparatively it seems that Hulu has the highest median score for its shows.

**Mean Ratings of Streamed Shows by Platform**

This graph shows the mean rotten tomatoes score of shows per platform. Netflix and Hulu seem to have almost the same mean rotten tomatoes score with Netflix slightly dominating. On the third place with rotten tomatoes score is Disney+, which is followed by Prime Vide that has the lowest mean score for shows streaming on it.

Overall, these exploratory data visualizations show that there is a difference between rotten tomatoes scores for shows on different platforms. Moving forward we explore the possible factors (exclusivity of the shows to each platform) that might affect these differences.

# Evaluation of significance

#### ***First Analysis: Hypothesis Test***

**Research Question:** Is the quality of exclusive shows different from non-exclusive shows in a specific platform, for each platform separately?

**Null Hypothesis**: The average Rotten Tomatoes score of exclusive shows is less than the average Rotten Tomatoes score of non-exclusive shows across all four platforms.

$$
H_0: \mu_{exclusive} < \mu_{non-exclusive}
$$

Alternative Hypothesis: The average Rotten Tomatoes score of exclusive shows is greater than the average Rotten Tomatoes score of non-exclusive shows across all four platforms.

$$
H_0: \mu_{exclusive} > \mu_{non-exclusive}
$$

```{r}
#|label: hypothesis-testing-analysis-1-multivariable
#|fig-height: 20

names(tv_shows_cleaned_copy)[7] <- "rotten_tomatoes"

observed_fit <- tv_shows_cleaned_copy |>
  specify(rotten_tomatoes ~ Netflix + Hulu + disney_plus + prime_video) |>
  fit()

observed_fit

null_fits <- tv_shows_cleaned_copy |>
  specify(rotten_tomatoes ~ Netflix + Hulu + disney_plus + prime_video) |>
  hypothesize(null = "independence") |>
  generate(reps = 1000, type = "permute") |>
  fit()

ci <- get_confidence_interval(
  x = null_fits,
  point_estimate = observed_fit,
  level = 0.95
)

print(ci)

get_p_value(
  x = null_fits,
  obs_stat = observed_fit,
  direction = "less"
)

visualize(null_fits) +
  shade_p_value(obs_stat = observed_fit, direction="two sided")

```

```{r}

tv_shows_cleaned_copy1 <- tv_shows_cleaned_copy |>
  pivot_longer(
    cols = exclusive_netflix:exclusive_disney_plus,
    names_to = "all_platforms",
    values_to = "exclusivity"
  )

tv_shows_cleaned_copy1

names(tv_shows_cleaned_copy1)[7] <- "rotten_tomatoes"

observed_fit1 <- tv_shows_cleaned_copy1 |>
  specify(rotten_tomatoes ~ exclusivity) |>
  calculate(stat = "diff in means", order = c(TRUE, FALSE))

observed_fit1

null_fits1 <- tv_shows_cleaned_copy1 |>
  specify(rotten_tomatoes ~ exclusivity) |>
  hypothesize(null = "independence") |>
  generate(reps = 1000, type = "permute") |>
  calculate(stat = "diff in means", order = c(TRUE, FALSE))

get_p_value(
  x = null_fits1,
  obs_stat = observed_fit1,
  direction = "less"
)

#Confidence Intervals

null_fits1 <- tv_shows_cleaned_copy1 |>
  specify(rotten_tomatoes ~ exclusivity) |>
  hypothesize(null = "independence") |>
  generate(reps = 1000, type = "bootstrap") |>
  calculate(stat = "diff in means", order = c(TRUE, FALSE))

ci1 <- get_confidence_interval(
  x = null_fits1,
  point_estimate = observed_fit1,
  level = 0.95
)

print(ci1)

visualize(null_fits1) +
  shade_p_value(obs_stat = observed_fit1, direction="less")

```

#### ***Second Analysis: Hypothesis Test***

**Research Question:** Are exclusive shows' ratings different between the four platforms (Netflix, Hulu, Disney+, and Prime Video)?

**Null Hypothesis**: There is no difference in quality of the exclusive shows between the four platforms.

$$
H_0: μ1 = μ2 = μ3 = μ4
$$

**Alternate Hypothesis:** There is a difference in quality of the exclusive shows between the four platforms.

$$
H_A: μ1 ≠ μ2 ≠ μ3 ≠ μ4
$$

```{r}
#| label: multivariate-hypothesis

library(forcats)

tv_shows_exclusive <- tv_shows_cleaned |>
  pivot_longer(cols = c(exclusive_netflix, exclusive_hulu, exclusive_disney_plus, exclusive_prime_video), values_to = "bool", names_to = "shows") |>
  filter(bool == TRUE) |>
  mutate(shows = fct_relevel(.f = shows, "exclusive_netflix"))

set.seed(123)
observed_fit <- tv_shows_exclusive |>
  specify(`Rotten Tomatoes` ~ shows) |>
  fit()

observed_fit


# Permuting to compute p values
null_fits <- tv_shows_exclusive |>
  specify(`Rotten Tomatoes` ~ shows) |>
  hypothesize(null = "independence") |>
  generate(reps = 1000, type = "permute") |>
  fit()

get_p_value(
  x = null_fits,
  obs_stat = observed_fit,
  direction = "two sided"
)

# Bootstrapping to compute confidence interval
h2_bootstrap <- tv_shows_exclusive |>
  specify(`Rotten Tomatoes` ~ shows) |>
  hypothesize(null = "independence") |>
  generate(reps = 1000, type = "bootstrap") |>
  fit()

get_confidence_interval(
  x = h2_bootstrap,
  point_estimate = observed_fit,
  level = .95)

visualize(null_fits) +
  shade_p_value(obs_stat = observed_fit, direction="two sided")
```

# Interpretation and conclusions

**Hypothesis #1: point-estimate**

The intercept of 35.45341513 shows, on average, the estimated Rotten Tomatoes rating for a show that is non-exclusive for any of the four platforms.

The coefficients for `NetflixYes`, `HuluYes`, `disney_plusYes`, and `prime_videoYes` represent the estimated differences in Rotten Tomatoes ratings that are exclusive to each platform compared to non-exclusive. The 17.02 coefficient for `NetflixYes` shows that shows exclusive to Netflix have an estimated average Rotten Tomato ratings higher by 17.02 points than ratings non-exclusive to Netflix. Similarly, the coefficient of `HuluYes` shows that shows exclusive to Hulu have an average of 15.79 points higher ratings in Rotten Tomatoes than non exclusive shows. The coefficient of `disney_plusYes` shows that shows exclusive to Disney+ have an average of 11.86 points higher Rotten Tomato ratings than non-exclusive shows. Finally, the coefficient of `prime_videoYes` shows that the shows exclusive to Prime Video have an average of 0.07 points lower ratings in Rotten Tomatoes than shows non-exclusive to Prime Video.

**Hypothesis #1: confidence intervals**

The 95% confidence intervals for observing similar results in the population for shows exclusive to either Hulu, Netflix, Disney+, or Prime Video are

-   \[-2.026, 1.82\] for `HuluYes`

-   \[-2.01, 1.83\] for `NetflixYes`

-   \[-2.56, 2.43\] for `disney_plusYes`

-   \[-2.07, 1.89\] for `prime_videoYes`

Because all the above confidence intervals include 0 in their interval we cannot confidently rule out the possibility that shows exclusive the given platform do not have a significant influence on Rotten Tomatoes ratings.

The 95% confidence interval \[45.36, 49.26\] for the intercept shows that we can be confident that the average Rotten Tomato rating observed in real population falls into the observed range.

**Hypothesis #1: p-values**

The intercept p-value is 0.001 which represents the p-value for non-exclusive shows of all 4 platforms. For exclusive shows on Hulu, Netflix, and Disney Plus, we calculated a p-value of 1 whereas exclusive shows on Prime Video had a p-value of 0.466. Since the p-values for exclusive shows on all 4 platforms is greater than the significance level indicated by $\alpha$ = 0.05, we fail to reject the null hypothesis (the mean Rotten Tomatoes score of exclusive shows is less than the mean Rotten Tomatoes score of high-rated non-exclusive shows across all four platforms). This means that there wasn't convincing evidence that would favor the alternative hypothesis.

**Hypothesis #2: point estimate**

The intercept 52.747303 represents, on average, the Rotten Tomatoes score for exclusive Netflix shows.

The estimated Rotten Tomatoes score for exclusive Disney plus shows, on average, will be 4.600244 less than the exclusive Netflix shows Rotten Tomatoes score. The estimated Rotten Tomatoes score for exclusive Hulu shows, on average, will be 1.556898 less than the exclusive Netflix shows Rotten Tomatoes score. The estimated Rotten Tomatoes score for exclusive Prime video shows, on average, will be 17.856883 less than the exclusive Netflix shows Rotten Tomatoes score

**Hypothesis #2: confidence intervals**

The 95% confidence intervals for observing the estimated Rotten Tomatoes scores for shows exclusive to either Hulu, Disney+, or Prime Video are

-   \[-2.347039, 2.210689\] for `exclusive_disney_plus`

-   \[-1.36853, 1.278844\] for `exclusive_hulu`

-   \[-1.362426, 1.294262\] for `exclusive_prime_video`

Since all the above confidence intervals include 0 within their interval, we cannot confidently rule out the possibility that shows exclusive to either Hulu, Disney+, or Prime Video do not have a significant influence on Rotten Tomatoes ratings.

The 95% confidence interval \[45.603521, 47.071124\] for the intercept shows that we can be confident that the estimated Rotten Tomato score for exclusive Netflix shows is within the given confidence interval range.

**Hypothesis #2: p-values**

The intercept p-value is 0.001 which represents the p-value for exclusive Netflix shows. For exclusive shows on Prime Video and Disney Plus, we calculated a p-value of 0.001 whereas exclusive shows on Hulu had a p-value of 0.02. Since the p-values for exclusive shows on all 4 platforms are less than the significance level indicated by $\alpha$ = 0.05, we are able to reject the null hypothesis (there is no difference in quality of the exclusive shows between the four platforms). This means that there was convincing evidence that would favor the alternative hypothesis which indicates that there is a difference in quality of the exclusive shows between the four platforms (measured through Rotten Tomatoes scores).

# Limitations

The first limitation is that our study right now just focuses on the quality of exclusive shows based on their rating scores (Rotten Tomatoes), which may not be a comprehensive measure of the quality of exclusive shows or non-exclusive shows on each platform. The analysis's focus on Rotten Tomatoes ratings alone may not fully capture the perceived quality of exclusive shows on each platform. Moreover, considering the ratings of exclusive shows and not non-exclusive shows could limit the overall understanding of the quality of each platform's content.

The second limitation is that we could not find any reliable and consistent dataset for the popularity, preferences, viewerships, viewing rates, or the number of subscribers of each platform, which could also affect the audiences' perceived quality and critical reception, limiting our study to find the relationship between the quality of exclusive shows or non-exclusive shows and the preference of platforms. This is a result of the competitiveness in the streaming industry. Companies' revenue almost entirely depends on subscriptions, and some of them are less active in publishing them. Additionally, some of the viewership data we find are using different metrics and, therefore, cannot be used to generate inferences. We can try to find a more comprehensive dataset that includes more variables which we could include in our analysis to reduce bias of confounding variables in hypothesis testing. We could also join dataset with those information.

The third limitation of our study is that there are unbalanced amounts of exclusive shows on each platform; for example, Netflix has significantly larger data of exclusive shows compared to the other platforms, which may affect the overall quality comparison between platforms. Disney Plus is a relatively new platform and has a smaller library of exclusive shows, which may limit the analysis. These unbalanced amounts of exclusive shows on each platform may introduce bias and affect the conclusions we can draw from our analyses.

The fourth limitation is that our analysis plan uses tomato scores to represent quality. This definition could have some biases. Firstly, the tomato score is a critics rating rather than an audience rating. Many movies and shows have a target audience that is better or worse represented than the other. Critics might inadvertently bring bias into their rating, and subsequently, our definition of quality could be biased as well. In addition, some shows that are watched by fewer people are less rated with tomato score, making the score easily influenced by a few critics, and therefore create bias in the definition of quality.

The fifth limitation is that our analysis does not account for the different types or genres of exclusive shows on each platform, such as comedies, documentaries, actions, etc., which may have different average ratings. We would have better insights if we had more data about the genres to compare shows of similar types across platforms rather than just looking at the overall average ratings.

# Acknowledgments

Thanks to this user who published the dataset we are using on Kraggle:

<https://www.kaggle.com/datasets/ruchi798/tv-shows-on-netflix-prime-video-hulu-and-disney>

\
